{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = '5'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "checkpoint_path = './pth'\n",
    "\n",
    "torch.set_num_threads(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.precision', 20)\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "class CFG:\n",
    "    batch_size=64\n",
    "    n_time = 3\n",
    "    total_n_bssid = 60\n",
    "    n_bssid = 40\n",
    "    n_tar = 1\n",
    "    ibeacon_seq_len = 20\n",
    "    n_sensor = 6\n",
    "    n_sensor_feature = 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dic_data_v1.pickle', 'rb') as f:\n",
    "    dic_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.81 s, sys: 9.04 ms, total: 5.82 s\n",
      "Wall time: 5.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import math\n",
    "for uid, dic_uid in dic_data.items():\n",
    "    l_idx, l_idx_sub = [], []\n",
    "    len_time = len(dic_uid['time'])\n",
    "#     len_time = int((dic_uid['y'][-1, 0] - dic_uid['y'][0, 0])/1e-7) + 2\n",
    "\n",
    "    t_int = np.linspace(dic_uid['y'][0, 0], dic_uid['y'][-1, 0], len_time)\n",
    "    y_int = np.zeros([len_time, 4])\n",
    "    y_int[:, 0] = t_int\n",
    "    y_int[:, 1] = dic_uid['y'][0, 1]\n",
    "    y_int[:, 2] = np.interp(t_int, dic_uid['y'][:, 0], dic_uid['y'][:, 2])\n",
    "    y_int[:, 3] = np.interp(t_int, dic_uid['y'][:, 0], dic_uid['y'][:, 3])    \n",
    "    dic_uid['y_int'] = y_int    \n",
    "\n",
    "    for tar, time_tar in enumerate(dic_uid['y_int'][:, 0]):\n",
    "        time_tar = t_int[tar]\n",
    "        s_wifi, e_wifi = get_se(dic_uid['time'], time_tar, CFG.n_time)\n",
    "        l_idx.append((uid, tar, s_wifi, e_wifi))\n",
    "\n",
    "    for tar, time_tar in enumerate(dic_uid['y'][:, 0]):\n",
    "        s_wifi, e_wifi = get_se(dic_uid['time'], time_tar, CFG.n_time)\n",
    "        l_idx_sub.append((uid, tar, s_wifi, e_wifi))\n",
    "            \n",
    "    dic_uid['l_idx'] = l_idx\n",
    "    dic_uid['l_idx_sub'] = l_idx_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_uid = np.array(list(dic_data.keys()))\n",
    "l_site = []\n",
    "for uid in ar_uid:\n",
    "    l_site.append(dic_data[uid]['site'])\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "stk = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "for idx_uid_train, idx_uid_valid in stk.split(ar_uid, l_site):\n",
    "    break\n",
    "\n",
    "l_idx_train, l_idx_valid = [], []\n",
    "\n",
    "for uid in ar_uid[idx_uid_train]:\n",
    "    l_idx_train += dic_data[uid]['l_idx']\n",
    "for uid in ar_uid[idx_uid_valid]:\n",
    "    l_idx_valid += dic_data[uid]['l_idx_sub']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231652, 7568)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_idx_train), len(l_idx_valid) # 이전 (67638, 7568)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0, 0, 3), (2, 1, 0, 3), (2, 2, 0, 3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_idx_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idx(s, e, is_test):\n",
    "    if is_test:\n",
    "        N = CFG.n_bssid\n",
    "    else:\n",
    "        N = CFG.total_n_bssid\n",
    "        \n",
    "    l = []\n",
    "    for i in range(s, e):\n",
    "        for j in range(N):\n",
    "            l.append((i, j))\n",
    "            \n",
    "    if is_test:\n",
    "        ar = np.array(l)\n",
    "    else:          \n",
    "        ar = np.array(l)[np.random.choice(range(len(l)), (e-s)*CFG.n_bssid, replace=False)]\n",
    "    \n",
    "    return ar[:, 0], ar[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 11000\n",
    "\n",
    "class ILNDataset(Dataset):\n",
    "    def __init__(self, l_idx, test=False):\n",
    "        self.l_idx = l_idx\n",
    "        self.test = test\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        u, t, s, e = self.l_idx[idx]  \n",
    "        dic_ = dic_data[u]\n",
    "\n",
    "        bssid = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        rssi = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        resp = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        time = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        lst = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        d_time = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        d_lst = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "\n",
    "        if self.test:\n",
    "            y = dic_['y'][t]            \n",
    "            idx_bssid = range(CFG.n_bssid)\n",
    "        else:\n",
    "            y = dic_['y_int'][t]\n",
    "            idx_bssid = np.random.choice(range(CFG.total_n_bssid), CFG.n_bssid, replace=False)\n",
    "            \n",
    "        length = (e-s) * CFG.n_bssid\n",
    "        bssid[:length] = dic_['bssid'][s:e, idx_bssid].flatten()\n",
    "        rssi[:length] = dic_['rssi'][s:e, idx_bssid].flatten()\n",
    "        resp[:length] = dic_['resp'][s:e, idx_bssid].flatten()\n",
    "        time[:length] = dic_['time'][s:e].repeat(CFG.n_bssid)\n",
    "        lst[:length] = dic_['lst'][s:e, idx_bssid].flatten()\n",
    "        d_time[:length] = dic_['time'][s:e].repeat(CFG.n_bssid) - y[0]\n",
    "        d_lst[:length] = dic_['lst'][s:e, idx_bssid].flatten() - y[0]\n",
    "        \n",
    "        mask_t = np.expand_dims(bssid!=0, -2) * 1\n",
    "        mask_b = bssid==0\n",
    "#         subsequent_mask = np.triu(np.ones((CFG.n_time, CFG.n_time)), k=1).astype('uint8') == 0  \n",
    "       \n",
    "        return bssid.astype(np.int64), rssi.astype(np.int64), resp.astype(np.float64), time.astype(np.float64), lst.astype(np.float64), \\\n",
    "    d_time.astype(np.float64), d_lst.astype(np.float64), y.astype(np.float64), mask_b, mask_t\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.l_idx)\n",
    "    \n",
    "\n",
    "train_db = ILNDataset(l_idx_train)\n",
    "valid_db = ILNDataset(l_idx_valid, test=True)\n",
    "\n",
    "train_loader = DataLoader(train_db, batch_size=CFG.batch_size, num_workers=1, shuffle=True)\n",
    "valid_loader = DataLoader(valid_db, batch_size=CFG.batch_size, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 11000\n",
    "\n",
    "class ILNDataset(Dataset):\n",
    "    def __init__(self, l_idx, test=False):\n",
    "        self.l_idx = l_idx\n",
    "        self.test = test\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        u, t, s, e = self.l_idx[idx]  \n",
    "        dic_ = dic_data[u]\n",
    "\n",
    "        bssid = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        rssi = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        resp = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        time = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        lst = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        d_time = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "        d_lst = np.zeros([CFG.n_time * CFG.n_bssid])\n",
    "\n",
    "        if self.test:\n",
    "            y = dic_['y'][t]            \n",
    "            idx_bssid = range(CFG.n_bssid)\n",
    "            \n",
    "            length = (e-s) * CFG.n_bssid\n",
    "            bssid[:length] = dic_['bssid'][s:e, idx_bssid].flatten()\n",
    "            rssi[:length] = dic_['rssi'][s:e, idx_bssid].flatten()\n",
    "            resp[:length] = dic_['resp'][s:e, idx_bssid].flatten()\n",
    "            time[:length] = dic_['time'][s:e].repeat(CFG.n_bssid)\n",
    "            lst[:length] = dic_['lst'][s:e, idx_bssid].flatten()\n",
    "            d_time[:length] = dic_['time'][s:e].repeat(CFG.n_bssid) - y[0]\n",
    "            d_lst[:length] = dic_['lst'][s:e, idx_bssid].flatten() - y[0]    \n",
    "            \n",
    "        else:\n",
    "            y = dic_['y_int'][t]\n",
    "#             idx_bssid = np.random.choice(range(CFG.total_n_bssid), CFG.n_bssid, replace=False)\n",
    "            idx_time = np.random.choice(range(s, e), CFG.n_time * CFG.n_bssid, replace=True)\n",
    "            idx_bssid = np.random.choice(range(CFG.total_n_bssid), CFG.n_time * CFG.n_bssid, replace=True)\n",
    "            \n",
    "            bssid = dic_['bssid'][idx_time, idx_bssid]\n",
    "            rssi = dic_['rssi'][idx_time, idx_bssid]\n",
    "            resp = dic_['resp'][idx_time, idx_bssid]\n",
    "            time = dic_['time'][idx_time]\n",
    "            lst = dic_['lst'][idx_time, idx_bssid]\n",
    "            d_time = dic_['time'][idx_time] - y[0]\n",
    "            d_lst = dic_['lst'][idx_time, idx_bssid] - y[0]\n",
    "        \n",
    "        mask_t = np.expand_dims(bssid!=0, -2) * 1\n",
    "        mask_b = bssid==0\n",
    "#         subsequent_mask = np.triu(np.ones((CFG.n_time, CFG.n_time)), k=1).astype('uint8') == 0  \n",
    "       \n",
    "        return bssid.astype(np.int64), rssi.astype(np.int64), resp.astype(np.float64), time.astype(np.float64), lst.astype(np.float64), \\\n",
    "    d_time.astype(np.float64), d_lst.astype(np.float64), y.astype(np.float64), mask_b, mask_t\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.l_idx)\n",
    "    \n",
    "\n",
    "train_db = ILNDataset(l_idx_train)\n",
    "valid_db = ILNDataset(l_idx_valid, test=True)\n",
    "\n",
    "train_loader = DataLoader(train_db, batch_size=CFG.batch_size, num_workers=1, shuffle=True)\n",
    "valid_loader = DataLoader(valid_db, batch_size=CFG.batch_size, num_workers=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0, 0, 3), (2, 1, 0, 3), (2, 2, 0, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_idx_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([60708, 56668, 24444, 53678, 11371, 18138, 55073, 64555, 45261,\n",
       "        60708, 60708, 17796, 35479,  4290, 44708, 11371, 45564, 21575,\n",
       "        56668,  2573, 49335, 45261, 61677, 23632, 60708, 41976, 18138,\n",
       "        45564, 15537,  2573, 60933, 14921,  8839, 15537, 53678, 24598,\n",
       "        44708, 50032, 50955,  8839, 43098, 23350, 43098,  3212, 30564,\n",
       "        46852, 35826,  4467, 37259, 23632, 49335, 45564, 35826, 34792,\n",
       "        62264, 11371, 45261, 61677,  8839, 53678, 17796, 48597, 14921,\n",
       "         8839, 31434,  9659, 60933, 60708,  3212, 29673, 55073, 14921,\n",
       "         3212, 30191, 24598, 25123, 37259, 26388, 48597, 55073, 64555,\n",
       "        43098, 31434, 44708, 55073, 56668, 61677, 15537,  4290, 60063,\n",
       "        23350, 24598, 15537, 41976, 17796,  4290, 61677, 52042, 57277,\n",
       "        35479, 46483, 45261,  3212, 23632, 43098, 41976, 30564, 13165,\n",
       "        44708,  2573, 11371, 24444, 63948, 18138, 10565, 23350,  1205,\n",
       "        56668, 46483, 50955]),\n",
       " array([37, 45, 37, 38, 34, 44, 50, 35, 31, 37, 41, 30, 34, 47, 31, 36, 33,\n",
       "        34, 45, 27, 36, 31, 38, 37, 46, 50, 45, 33, 30, 27, 41, 46, 39, 32,\n",
       "        38, 29, 31, 37, 32, 42, 30, 27, 35, 29, 38, 41, 36, 28, 27, 37, 38,\n",
       "        33, 36, 26, 38, 37, 30, 41, 42, 38, 29, 31, 46, 42, 27, 27, 41, 37,\n",
       "        26, 44, 48, 46, 29, 46, 34, 34, 27, 39, 31, 43, 36, 30, 27, 31, 43,\n",
       "        32, 41, 30, 37, 39, 37, 31, 30, 48, 29, 47, 41, 31, 55, 36, 45, 34,\n",
       "        29, 37, 35, 50, 38, 41, 31, 27, 34, 37, 27, 39, 37, 37, 36, 45, 44,\n",
       "        31]),\n",
       " array([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "        1.]),\n",
       " array([1.84639705, 1.84639686, 1.84639686, 1.84639705, 1.84639705,\n",
       "        1.84639686, 1.84639686, 1.84639705, 1.84639686, 1.84639705,\n",
       "        1.84639686, 1.84639725, 1.84639705, 1.84639725, 1.84639686,\n",
       "        1.84639725, 1.84639705, 1.84639725, 1.84639686, 1.84639686,\n",
       "        1.84639725, 1.84639686, 1.84639705, 1.84639725, 1.84639725,\n",
       "        1.84639686, 1.84639705, 1.84639705, 1.84639725, 1.84639686,\n",
       "        1.84639725, 1.84639705, 1.84639705, 1.84639705, 1.84639705,\n",
       "        1.84639725, 1.84639705, 1.84639705, 1.84639686, 1.84639686,\n",
       "        1.84639705, 1.84639686, 1.84639686, 1.84639705, 1.84639725,\n",
       "        1.84639705, 1.84639686, 1.84639725, 1.84639725, 1.84639705,\n",
       "        1.84639705, 1.84639705, 1.84639686, 1.84639686, 1.84639705,\n",
       "        1.84639686, 1.84639725, 1.84639686, 1.84639686, 1.84639705,\n",
       "        1.84639705, 1.84639725, 1.84639686, 1.84639686, 1.84639705,\n",
       "        1.84639725, 1.84639725, 1.84639705, 1.84639686, 1.84639725,\n",
       "        1.84639705, 1.84639725, 1.84639705, 1.84639686, 1.84639705,\n",
       "        1.84639705, 1.84639725, 1.84639686, 1.84639686, 1.84639725,\n",
       "        1.84639725, 1.84639705, 1.84639686, 1.84639725, 1.84639725,\n",
       "        1.84639725, 1.84639686, 1.84639686, 1.84639705, 1.84639705,\n",
       "        1.84639705, 1.84639686, 1.84639686, 1.84639705, 1.84639705,\n",
       "        1.84639725, 1.84639686, 1.84639705, 1.84639725, 1.84639725,\n",
       "        1.84639686, 1.84639705, 1.84639705, 1.84639686, 1.84639686,\n",
       "        1.84639686, 1.84639725, 1.84639686, 1.84639705, 1.84639686,\n",
       "        1.84639705, 1.84639686, 1.84639686, 1.84639725, 1.84639686,\n",
       "        1.84639705, 1.84639725, 1.84639686, 1.84639705, 1.84639705]),\n",
       " array([1.846397  , 1.84639526, 1.84639678, 1.84639689, 1.84639697,\n",
       "        1.84639668, 1.84639679, 1.84639697, 1.84639675, 1.846397  ,\n",
       "        1.84639681, 1.84639717, 1.84639697, 1.84639719, 1.84639524,\n",
       "        1.84639717, 1.84639696, 1.84639709, 1.84639526, 1.84639529,\n",
       "        1.84639709, 1.84639675, 1.846397  , 1.84639678, 1.84639719,\n",
       "        1.84639679, 1.84639687, 1.84639696, 1.84639714, 1.84639529,\n",
       "        1.84639718, 1.84639674, 1.84639687, 1.84639694, 1.84639689,\n",
       "        1.84639714, 1.84639524, 1.846397  , 1.84639675, 1.84639523,\n",
       "        1.84639689, 1.8463967 , 1.84639526, 1.84639698, 1.84639707,\n",
       "        1.84639687, 1.8463967 , 1.8463969 , 1.8463953 , 1.84639678,\n",
       "        1.84639689, 1.84639696, 1.8463967 , 1.84639679, 1.846397  ,\n",
       "        1.84639678, 1.84639714, 1.84639681, 1.84639523, 1.84639689,\n",
       "        1.84639698, 1.84639714, 1.84639674, 1.84639523, 1.84639413,\n",
       "        1.84639523, 1.84639718, 1.846397  , 1.84639679, 1.84639712,\n",
       "        1.84639699, 1.84639674, 1.84639698, 1.84639671, 1.84639694,\n",
       "        1.8463969 , 1.8463953 , 1.84639671, 1.84639675, 1.84639718,\n",
       "        1.84639717, 1.84639689, 1.84639413, 1.84639524, 1.84639718,\n",
       "        1.84639709, 1.84639681, 1.84639675, 1.846397  , 1.84639691,\n",
       "        1.84639689, 1.84639675, 1.84639675, 1.84639699, 1.84639698,\n",
       "        1.84639719, 1.84639681, 1.84639674, 1.84639712, 1.84639717,\n",
       "        1.84639673, 1.84639694, 1.84639698, 1.84639678, 1.84639526,\n",
       "        1.84639679, 1.84639707, 1.84639681, 1.84639524, 1.84639529,\n",
       "        1.84639697, 1.84639678, 1.84639471, 1.84639706, 1.84639525,\n",
       "        1.84639689, 1.84639717, 1.84639526, 1.84639693, 1.84639694]),\n",
       " array([3.853e-07, 1.919e-07, 1.919e-07, 3.853e-07, 3.853e-07, 1.919e-07,\n",
       "        1.919e-07, 3.853e-07, 1.919e-07, 3.853e-07, 1.919e-07, 5.781e-07,\n",
       "        3.853e-07, 5.781e-07, 1.919e-07, 5.781e-07, 3.853e-07, 5.781e-07,\n",
       "        1.919e-07, 1.919e-07, 5.781e-07, 1.919e-07, 3.853e-07, 5.781e-07,\n",
       "        5.781e-07, 1.919e-07, 3.853e-07, 3.853e-07, 5.781e-07, 1.919e-07,\n",
       "        5.781e-07, 3.853e-07, 3.853e-07, 3.853e-07, 3.853e-07, 5.781e-07,\n",
       "        3.853e-07, 3.853e-07, 1.919e-07, 1.919e-07, 3.853e-07, 1.919e-07,\n",
       "        1.919e-07, 3.853e-07, 5.781e-07, 3.853e-07, 1.919e-07, 5.781e-07,\n",
       "        5.781e-07, 3.853e-07, 3.853e-07, 3.853e-07, 1.919e-07, 1.919e-07,\n",
       "        3.853e-07, 1.919e-07, 5.781e-07, 1.919e-07, 1.919e-07, 3.853e-07,\n",
       "        3.853e-07, 5.781e-07, 1.919e-07, 1.919e-07, 3.853e-07, 5.781e-07,\n",
       "        5.781e-07, 3.853e-07, 1.919e-07, 5.781e-07, 3.853e-07, 5.781e-07,\n",
       "        3.853e-07, 1.919e-07, 3.853e-07, 3.853e-07, 5.781e-07, 1.919e-07,\n",
       "        1.919e-07, 5.781e-07, 5.781e-07, 3.853e-07, 1.919e-07, 5.781e-07,\n",
       "        5.781e-07, 5.781e-07, 1.919e-07, 1.919e-07, 3.853e-07, 3.853e-07,\n",
       "        3.853e-07, 1.919e-07, 1.919e-07, 3.853e-07, 3.853e-07, 5.781e-07,\n",
       "        1.919e-07, 3.853e-07, 5.781e-07, 5.781e-07, 1.919e-07, 3.853e-07,\n",
       "        3.853e-07, 1.919e-07, 1.919e-07, 1.919e-07, 5.781e-07, 1.919e-07,\n",
       "        3.853e-07, 1.919e-07, 3.853e-07, 1.919e-07, 1.919e-07, 5.781e-07,\n",
       "        1.919e-07, 3.853e-07, 5.781e-07, 1.919e-07, 3.853e-07, 3.853e-07]),\n",
       " array([ 3.29000000e-07, -1.41080000e-06,  1.12600000e-07,  2.22600000e-07,\n",
       "         3.04100000e-07,  6.60000010e-09,  1.24000000e-07,  3.04100000e-07,\n",
       "         8.22999999e-08,  3.29000000e-07,  1.37600000e-07,  5.04100000e-07,\n",
       "         3.04200000e-07,  5.22700000e-07, -1.43190000e-06,  4.97800000e-07,\n",
       "         2.91600000e-07,  4.21100000e-07, -1.41080000e-06, -1.37880000e-06,\n",
       "         4.19200000e-07,  8.22999999e-08,  3.28900000e-07,  1.12700000e-07,\n",
       "         5.22900000e-07,  1.21700000e-07,  2.01100000e-07,  2.91600000e-07,\n",
       "         4.68800000e-07, -1.37880000e-06,  5.10300000e-07,  6.92000000e-08,\n",
       "         2.00600000e-07,  2.75200000e-07,  2.22600000e-07,  4.68800000e-07,\n",
       "        -1.43190000e-06,  3.28800000e-07,  8.14000001e-08, -1.43930000e-06,\n",
       "         2.23600000e-07,  2.98000000e-08, -1.41290000e-06,  3.10500000e-07,\n",
       "         3.97600000e-07,  1.97800000e-07,  2.67000000e-08,  2.31600000e-07,\n",
       "        -1.36610000e-06,  1.12700000e-07,  2.22400000e-07,  2.91600000e-07,\n",
       "         2.67000000e-08,  1.19700000e-07,  3.28800000e-07,  1.12700000e-07,\n",
       "         4.69600000e-07,  1.37400000e-07, -1.43930000e-06,  2.22600000e-07,\n",
       "         3.10400000e-07,  4.69200000e-07,  6.92000000e-08, -1.43930000e-06,\n",
       "        -2.54110000e-06, -1.44140000e-06,  5.10300000e-07,  3.29000000e-07,\n",
       "         1.19800000e-07,  4.51900000e-07,  3.16400000e-07,  6.92000000e-08,\n",
       "         3.10500000e-07,  3.79000000e-08,  2.75200000e-07,  2.28500000e-07,\n",
       "        -1.36610000e-06,  3.82000001e-08,  8.21999999e-08,  5.10200000e-07,\n",
       "         4.97800000e-07,  2.23600000e-07, -2.54110000e-06, -1.43190000e-06,\n",
       "         5.10200000e-07,  4.24100000e-07,  1.37400000e-07,  7.85000001e-08,\n",
       "         3.28900000e-07,  2.42900000e-07,  2.22100000e-07,  8.22999999e-08,\n",
       "         7.85000001e-08,  3.16200000e-07,  3.10400000e-07,  5.22700000e-07,\n",
       "         1.37400000e-07,  6.97000000e-08,  4.55900000e-07,  4.97900000e-07,\n",
       "         6.55000001e-08,  2.75200000e-07,  3.10500000e-07,  1.12700000e-07,\n",
       "        -1.41290000e-06,  1.21700000e-07,  3.97600000e-07,  1.37500000e-07,\n",
       "        -1.43190000e-06, -1.37880000e-06,  3.04100000e-07,  1.12600000e-07,\n",
       "        -1.95680000e-06,  3.89100000e-07, -1.41660000e-06,  2.22100000e-07,\n",
       "         4.98000000e-07, -1.41080000e-06,  2.60000000e-07,  2.75100000e-07]),\n",
       " array([  1.84639667,   1.        , 198.36833   , 163.52063   ]),\n",
       " array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False]),\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, Any\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Module, MultiheadAttention, ModuleList, Dropout, Linear, Linear, LayerNorm\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "class TransformerEncoder(Module):\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        output = src\n",
    "\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "#         if self.norm is not None:\n",
    "#             output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "#         src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "#         src = self.norm2(src)\n",
    "        return src\n",
    "    \n",
    "    \n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "    \n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_bssid = 128\n",
    "dim_rssi = 16\n",
    "dim_time = 1\n",
    "d_model = 128\n",
    "max_bssid = 239312 # df_100.bssid.max() + 1\n",
    "max_rssi = 110\n",
    "# max_b_id = 7020\n",
    "\n",
    "class Transformer(Module):\n",
    "    def __init__(self, d_model: int = d_model, nhead: int = 8, num_encoder_layers: int = 4,\n",
    "                 dim_feedforward: int = d_model*4, dropout: float = 0.0, activation: str = \"relu\"):\n",
    "        super(Transformer, self).__init__()\n",
    "                 \n",
    "        self.emb_bssid = nn.Embedding(max_bssid, dim_bssid)\n",
    "        self.emb_rssi = nn.Embedding(max_rssi, dim_rssi)\n",
    "        \n",
    "#         self.norm = nn.LayerNorm(dim_bssid+dim_rssi+dim_time*3)\n",
    "        self.v = nn.Linear(dim_bssid+dim_rssi+5, d_model)     \n",
    "        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, activation)\n",
    "        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, nn.LayerNorm(d_model))\n",
    "        self._reset_parameters()          \n",
    "\n",
    "        self.w = nn.Linear(d_model, CFG.n_tar * 3)  \n",
    "                 \n",
    "    def forward(self, bssid, rssi, resp, time, lst, d_time, d_lst, y, mask_b, mask_t):\n",
    "        bssid = self.emb_bssid(bssid).type(torch.float64)\n",
    "        rssi = self.emb_rssi(rssi).type(torch.float64)\n",
    "        resp, time, lst, d_time, d_lst = resp.unsqueeze(-1), time.unsqueeze(-1), lst.unsqueeze(-1), d_time.unsqueeze(-1), d_lst.unsqueeze(-1)\n",
    "        x = torch.cat([bssid, rssi, resp, time, lst, d_time, d_lst], dim=-1)\n",
    "        \n",
    "#         x = self.norm(x)\n",
    "        x = self.v(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.encoder(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = x[:, -1] \n",
    "\n",
    "        x = self.w(x)         \n",
    "        return x\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask    \n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_batch = 4\n",
    "bssid = torch.zeros([size_batch, CFG.n_time*CFG.n_bssid], dtype=torch.int64)\n",
    "rssi = torch.zeros([size_batch, CFG.n_time*CFG.n_bssid], dtype=torch.int64)\n",
    "resp = torch.zeros([size_batch, CFG.n_time*CFG.n_bssid], dtype=torch.float64)\n",
    "time = torch.zeros([size_batch, CFG.n_time*CFG.n_bssid], dtype=torch.float64)\n",
    "lst = torch.zeros([size_batch, CFG.n_time*CFG.n_bssid], dtype=torch.float64)\n",
    "d_time = torch.zeros([size_batch, CFG.n_time*CFG.n_bssid], dtype=torch.float64)\n",
    "d_lst = torch.zeros([size_batch, CFG.n_time*CFG.n_bssid], dtype=torch.float64)\n",
    "y = torch.zeros([size_batch, 4])\n",
    "\n",
    "mask_b = torch.ones([size_batch, CFG.n_bssid]) == 0\n",
    "mask_t = torch.ones([size_batch, 1, CFG.n_time])\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "Transformer()(bssid, rssi, resp, time, lst, d_time, d_lst, y, mask_b, mask_t).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def unitwise_norm(x, norm_type=2.0):\n",
    "    if x.ndim <= 1:\n",
    "        return x.norm(norm_type)\n",
    "    else:\n",
    "        # works for nn.ConvNd and nn.Linear where output dim is first in the kernel/weight tensor\n",
    "        # might need special cases for other weights (possibly MHA) where this may not be true\n",
    "#         return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n",
    "        return x.norm(norm_type, dim=tuple(range(0, x.ndim)), keepdim=True)\n",
    "\n",
    "\n",
    "def adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    for p in parameters:\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        p_data = p.detach()\n",
    "        g_data = p.grad.detach()\n",
    "        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n",
    "        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n",
    "        clipped_grad = g_data * (max_norm / grad_norm.clamp(min=1e-6))\n",
    "        new_grads = torch.where(grad_norm < max_norm, g_data, clipped_grad)\n",
    "        p.grad.detach().copy_(new_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iln_loss(pred, y):\n",
    "    return torch.mean(torch.abs(pred[:, 0] - y[:, 1]) * 15 + ((pred[:, 1] - y[:, 2]) ** 2 + (pred[:, 2] - y[:, 3]) ** 2) ** 0.5)\n",
    "\n",
    "def iln_loss_valid(pred, y_int, y_t):\n",
    "    pred, y_int, y_t = pred.detach().cpu().numpy(), y_int.detach().cpu().numpy(), y_t.detach().cpu().numpy()\n",
    "    ar = np.zeros((len(y_t), 3))\n",
    "    for i in range(len(y_t)):\n",
    "        ar[i, 0] = np.interp(y_t[i, 0], y_int[i, :, 0], pred[i, :, 0])\n",
    "        ar[i, 1] = np.interp(y_t[i, 0], y_int[i, :, 0], pred[i, :, 1])\n",
    "        ar[i, 2] = np.interp(y_t[i, 0], y_int[i, :, 0], pred[i, :, 2])\n",
    "        \n",
    "    return np.mean(np.abs(ar[:, 0] - y_t[:, 1]) * 15 + ((ar[:, 1] - y_t[:, 2]) ** 2 + (ar[:, 2] - y_t[:, 3]) ** 2) ** 0.5)\n",
    "\n",
    "# def iln_loss(pred, y):\n",
    "#     return torch.mean(torch.abs(pred[:, 0] - y[:, 1]) * 15 + ((pred[:, 1] - y[:, 2]) ** 2 + (pred[:, 2] - y[:, 3]) ** 2) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(dataloaders, model, is_train):\n",
    "    start = time.time()\n",
    "    l_train_loss, l_valid_loss, l_tar, l_pred = [], [], [], []\n",
    "    \n",
    "    for i, (bssid, rssi, resp, t, lst, d_t, d_lst, y, mask_b, mask_t) in enumerate(dataloaders):\n",
    "        bssid, rssi, resp, t, lst, d_t, d_lst, y, mask_b, mask_t \\\n",
    "        = bssid.cuda(), rssi.cuda(), resp.cuda(), t.cuda(), lst.cuda(), d_t.cuda(), d_lst.cuda(), y.cuda(), mask_b.cuda(), mask_t.cuda()\n",
    "        \n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            pred = model(bssid, rssi, resp, t, lst, d_t, d_lst, y, mask_b, mask_t)\n",
    "            loss = iln_loss(pred, y)\n",
    "            l_train_loss.append(loss.detach().cpu().numpy())\n",
    "            \n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                adaptive_clip_grad(model.parameters())\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                if i % 1000 == 0:\n",
    "                    print('Step: %d Loss: %.4f Time: %.0f' %(i, np.array(l_train_loss).mean(), time.time()-start))\n",
    "                    start = time.time()                \n",
    "            else:\n",
    "                loss = iln_loss(pred, y)\n",
    "                l_valid_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    train_loss = np.array(l_train_loss).mean()\n",
    "    \n",
    "    if is_train:\n",
    "        return train_loss\n",
    "    else:\n",
    "        valid_loss = np.array(l_valid_loss).mean()\n",
    "        return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "hist_loss_train, hist_loss_valid = {}, {}\n",
    "\n",
    "def run():\n",
    "    best_loss, best_epoch = 100, 0\n",
    "    for epoch in range(1000):\n",
    "        t = time.time()\n",
    "        _ = model.train()\n",
    "        train_loss = run_epoch(train_loader, model, True)\n",
    "        hist_loss_train[epoch] = train_loss\n",
    "        print('Epoch: %d Loss: %.4f Time: %0.f' %(epoch, train_loss, time.time()-t))\n",
    "        \n",
    "        t = time.time()\n",
    "        _ = model.eval()        \n",
    "        train_loss, valid_loss = run_epoch(valid_loader, model, False)\n",
    "        hist_loss_valid[epoch] = valid_loss\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "#             torch.save(model.state_dict(), checkpoint_path)\n",
    "            best_epoch = epoch\n",
    "            best_loss = valid_loss\n",
    "        print('Epoch: %d V_Loss: %.4f Best: %.4f %d' %(epoch, valid_loss, best_loss, best_epoch))\n",
    "#         print('Epoch: %d V_Loss: %.4f Best: %.4f %d lr: %.6f' %(epoch, valid_loss, best_loss, best_epoch, scheduler.get_last_lr()[0]))\n",
    "        print('')        \n",
    "        \n",
    "#         scheduler.step(valid_loss)\n",
    "#         scheduler.step()\n",
    "        \n",
    "    return best_epoch, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (emb_bssid): Embedding(239312, 128)\n",
       "  (emb_rssi): Embedding(110, 16)\n",
       "  (v): Linear(in_features=149, out_features=128, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (w): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=0.0003) \n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min')\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=20, gamma=0.9)\n",
    "model.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
